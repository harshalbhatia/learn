{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "f67729355a01c19c62bfa2145d9594b1da821fc6"
   },
   "outputs": [],
   "source": [
    "!pip install -q six numpy scipy matplotlib scikit-image opencv-python imageio\n",
    "!pip install -q keras imgaug\n",
    "!pip install -q keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from imgaug import augmenters as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "363049cf0704e0aebfa7ea4994c6c291fd66cbac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import *\n",
    "from keras.models import load_model\n",
    "\n",
    "# from keras.callbacks import LearningRateScheduler,ModelCheckpoint,EarlyStopping,LambdaCallback\n",
    "import os,sys,math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "22f446b53ec82f3244c33dd780ab24b896178cf9"
   },
   "outputs": [],
   "source": [
    "import google\n",
    "colab_dir='./'\n",
    "file_name='EIP_CIFAR_10'\n",
    "if hasattr(google,'colab'):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    colab_dir='/content/gdrive/My Drive/Colab Notebooks/'\n",
    "model_file=colab_dir+file_name+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "aa9bb45cb7f0171c15c4b065ade989e82042b15a"
   },
   "outputs": [],
   "source": [
    "#Augmentation and resizing\n",
    "def augment(dataset,flip=0.65,blur=2.0,crop=(0,10)):\n",
    "    seq = ia.Sequential([ia.Fliplr(flip),ia.GaussianBlur(sigma=(0, blur)),ia.Crop(px=crop)])\n",
    "    return seq.augment_images(dataset)\n",
    "\n",
    "def augmenter(X,y,start=.25,end=.75):\n",
    "    ln=len(X)\n",
    "    print('Before augmentation:',X.shape,y.shape)\n",
    "    start=int(start*ln)\n",
    "    end=int(end*ln)\n",
    "    new_X=augment(X)[start:end]\n",
    "    new_y=y[start:end]\n",
    "    X=np.concatenate((X,new_X))\n",
    "    y=np.concatenate((y,new_y))\n",
    "    print('After augmentation:',X.shape,y.shape)\n",
    "    return (X,y)\n",
    "\n",
    "def resize_imgs(imgs,shape=(26,26)):\n",
    "    seq = ia.Sequential([ia.Scale({\"height\": shape[0], \"width\": shape[1]})])\n",
    "    return seq.augment_images(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b55206328faf39cfbc450beddc20dc5a90aa5218"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2,num_layers=12):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(num_layers):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])        \n",
    "        temp = concat        \n",
    "    return temp\n",
    "\n",
    "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "        Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)    \n",
    "    return avg\n",
    "\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)    \n",
    "    return output\n",
    "  \n",
    "def dense_unit(input_layer,num_filter, dropout_rate,num_layers):\n",
    "    dense_block=add_denseblock(input_layer, num_filter, dropout_rate,num_layers)\n",
    "    transition_block=add_transition(dense_block, num_filter, dropout_rate)\n",
    "    return transition_block\n",
    "  \n",
    "def dense_units_chain(n,input_layer,num_filter, dropout_rate,num_layers):\n",
    "    dense_unit_=input_layer\n",
    "    for i in range(n):\n",
    "        dense_unit_=dense_unit(dense_unit_,num_filter, dropout_rate,num_layers)\n",
    "    return dense_unit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d104b63cd5b877c9d8acd0a7ac991f17befa41ea"
   },
   "outputs": [],
   "source": [
    "#training time ops\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,new_step_size=None):\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1       \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "85afcd30b9f0ca412bcf7b04938730e52e04bb47"
   },
   "outputs": [],
   "source": [
    "def load_prev_model(model_small=None):\n",
    "#     print(os.popen('du -sh '+colab_dir+'*').read())\n",
    "    last_best=os.popen('du -sh '+colab_dir+r'*|sed -r \"s/^[0-9\\.]+[MK]?\\s(.*)$/\\1/g;\"|tail -1').read().strip()\n",
    "    model_prev=None\n",
    "    try:\n",
    "        print('Attempting to load last best model from file - ',end='')\n",
    "        last_best_fn=last_best.split('/')[-1]\n",
    "        last_best_epoch=last_best_fn.split('.')[1].split('-')[0]\n",
    "        model_prev=load_model(last_best)\n",
    "        print('Sucess\\nLoaded '+last_best_fn, 'epoch :', last_best_epoch)\n",
    "    except Exception as e:\n",
    "        print('Failed!\\n',e)\n",
    "        try:\n",
    "            print('Attempting to load last saved model from file - ',end='')\n",
    "            model_prev = load_model(model_file)\n",
    "            print('Sucess\\nLoaded model from file',model_file)\n",
    "        except Exception as e:\n",
    "    #         print(str(e), 'at line ', sys.exc_info()[2].tb_lineno)\n",
    "            print('Failed!\\n',e)\n",
    "            try:\n",
    "                print('Attempting to load in memory, small model - ',end='')\n",
    "                if len(model_small.layers)>1:\n",
    "                    model_prev=model_small\n",
    "                print('Sucess')\n",
    "            except Exception as e:\n",
    "                print('Failed!\\n',e)\n",
    "    return model_prev\n",
    "def try_and_copy_weights(model_to,model_from):\n",
    "    #model_to.set_weights(model_from.get_weights())\n",
    "    s,err=0,0\n",
    "    print('Trying to copy weights')\n",
    "    try:\n",
    "        if len(model_to.layers) >1 and len(model_from.layers) >1:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print('Inavlid models',model_to,model_from)\n",
    "        return\n",
    "    for new_layer, layer in zip(model_to.layers[1:], model_from.layers[1:]):\n",
    "        s+=1\n",
    "        try:\n",
    "            new_layer.set_weights(layer.get_weights())\n",
    "            print('.',end='')\n",
    "        except Exception as e:\n",
    "            err+=1\n",
    "    print('Success:',s-err,'Layers , error:',err,'Layers')\n",
    "    # if model_prev.input_shape[1:]==smaller_input:\n",
    "#     try:\n",
    "#         print('Popping input layer ... ',end='')\n",
    "#         model_prev.layers.pop(0)\n",
    "#         print('Successful')\n",
    "#     except Exception as e:\n",
    "#         print('Failed!\\n',e)\n",
    "# model_large.set_weights(model_prev.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "9d5f083dfa8904a9bd7433e385ad2a0b8a236b22"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "def load_data(resize=False,shape=(26,26),test_augment=False):\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    if resize:\n",
    "        x_train=resize_imgs(x_train,shape)\n",
    "        x_test=resize_imgs(x_test,shape)\n",
    "    \n",
    "    (x_train, y_train) = augmenter(x_train, y_train,end=1)\n",
    "    if test_augment:\n",
    "        (x_test, y_test) = augmenter(x_test, y_test,end=1)\n",
    "    return (x_train, y_train,x_test, y_test)\n",
    "#create a dnn model\n",
    "def create_model(input_shape,num_layers,input=None):\n",
    "    print('Creating model with input shape',input_shape)\n",
    "    if input is None:\n",
    "        input = Input(input_shape,input)\n",
    "    First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "    hidden_dense_blocks = dense_units_chain(n_dense_blocks,First_Conv2D,num_filter,dropout_rate,num_layers)\n",
    "    Last_Block = add_denseblock(hidden_dense_blocks, num_filter, dropout_rate)\n",
    "    output = output_layer(Last_Block)\n",
    "    model = Model(inputs=[input], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "991f8e371577a1f2e8197827dab5aac2406ef1b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================HYPER PARAMETERS====================\n"
     ]
    }
   ],
   "source": [
    "print('====================HYPER PARAMETERS====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "02c643e295c0bc677da79c821a40e5b15cc3fdaf"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "max_epochs = 250\n",
    "model_small_epochs=10\n",
    "model_large_epochs=40\n",
    "\n",
    "num_layers = 24\n",
    "# layers_small=12\n",
    "# layers_large=num_layers-layers_small\n",
    "layers_large=layers_small=num_layers\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2\n",
    "n_dense_blocks = 3\n",
    "smaller_input=(26,26,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "275d54ceb5bde42b199b74a7b7fa0b0677874feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================BEGIN OF SMALLER MODEL====================\n"
     ]
    }
   ],
   "source": [
    "print('====================BEGIN OF SMALLER MODEL====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "4fec52f1aaca7cc57b2152e1b128ed082a482e1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before augmentation: (50000, 26, 26, 3) (50000, 10)\n",
      "After augmentation: (87500, 26, 26, 3) (87500, 10)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "x_train, y_train, x_test, y_test = load_data(resize=True,shape=smaller_input[:-1])\n",
    "\n",
    "#callbacks\n",
    "model_checkpointer=ModelCheckpoint('small_weights.{epoch:02d}-{val_acc:.2f}.h5', monitor='val_acc',\n",
    "                verbose=1, save_best_only=True, save_weights_only=False, mode='max', period=2)\n",
    "# lrate = LearningRateScheduler(step_decay)\n",
    "# early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "clr = CyclicLR(base_lr=0.1, max_lr=0.2,step_size=8*(len(y_train)/batch_size))\n",
    "callbacks = [clr, model_checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "a1c8f577fd7d55a94eea1620db6e4a4d0356ec81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model with input shape (26, 26, 3)\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model_small = create_model(x_train.shape[1:],num_layers=layers_small)\n",
    "model_small.compile(loss='categorical_crossentropy',metrics=['accuracy'],\n",
    "              optimizer=SGD(lr=0.1, decay=1e-6, momentum=0.8, nesterov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4aa8818828844c3c353d304c2dd84b2b3e68024",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87500 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "87500/87500 [==============================] - 262s 3ms/step - loss: 1.7972 - acc: 0.3351 - val_loss: 2.4088 - val_acc: 0.2599\n",
      "Epoch 2/10\n",
      "87500/87500 [==============================] - 242s 3ms/step - loss: 1.6000 - acc: 0.4113 - val_loss: 1.6955 - val_acc: 0.4324\n",
      "\n",
      "Epoch 00002: val_acc improved from -inf to 0.43240, saving model to small_weights.02-0.43.h5\n",
      "Epoch 3/10\n",
      "16500/87500 [====>.........................] - ETA: 3:09 - loss: 1.5177 - acc: 0.4415"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "model_small.fit(x_train, y_train, batch_size=batch_size,verbose=1,\n",
    "                    epochs=model_small_epochs, callbacks=callbacks,\n",
    "                    validation_data=(x_test, y_test))\n",
    "model_small.save(model_file)\n",
    "print('Saved model_small to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86607351bf8f2b7358482e908fe190924650691a"
   },
   "outputs": [],
   "source": [
    "print('====================END OF SMALLER MODEL====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80cc5c901ceb489ffb260bff35d50ee9af6b1826"
   },
   "outputs": [],
   "source": [
    "print('====================BEGIN OF LARGER MODEL====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c17f4244c917e97e7cc1557ae60bce8dba7e47e"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "x_train, y_train, x_test, y_test = load_data(resize=False)\n",
    "\n",
    "#callbacks\n",
    "model_checkpointer=ModelCheckpoint('large_weights.{epoch:02d}-{val_acc:.2f}.h5', monitor='val_acc',\n",
    "                verbose=1, save_best_only=True, save_weights_only=False, mode='max', period=2)\n",
    "# lrate = LearningRateScheduler(step_decay)\n",
    "# early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "clr = CyclicLR(base_lr=0.1, max_lr=0.2,step_size=2000, mode='exp_range',gamma=0.99994)\n",
    "callbacks = [clr, model_checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "710f2dd11efc0fb7346ad3c15dd4441addafaeb7"
   },
   "outputs": [],
   "source": [
    "model_large.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f3adc0c3037f0d323cda32709d8abd57602b3c3"
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "model_prev=load_prev_model(model_small)\n",
    "\n",
    "# model_large = create_model(x_train.shape[1:],num_layers=layers_large,input=model_prev.output)\n",
    "model_large = create_model(x_train.shape[1:],num_layers=num_layers)\n",
    "#model_large.set_weights(model_prev.get_weights())\n",
    "try_and_copy_weights(model_large,model_prev)\n",
    "\n",
    "model_large.compile(loss='categorical_crossentropy',metrics=['accuracy'],\n",
    "                    optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.8, nesterov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53b7ed0b0e8acefefda51802f0bd74f0cace4c72"
   },
   "outputs": [],
   "source": [
    "#train model\n",
    "model_large.fit(x_train, y_train, batch_size=batch_size, verbose=1,\n",
    "                epochs=model_large_epochs, callbacks=callbacks,\n",
    "                validation_data=(x_test, y_test))\n",
    "model_large.save(model_file)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7be8a1352f65787da27dc9eeb8b9073890561062"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "score = model_large.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6151d97fb488aed1553bb9ae29fcb9e3e0bf1f08"
   },
   "outputs": [],
   "source": [
    "print('====================END OF LARGER MODEL====================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
