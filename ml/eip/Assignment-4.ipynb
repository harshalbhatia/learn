{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f67729355a01c19c62bfa2145d9594b1da821fc6"
      },
      "cell_type": "code",
      "source": "# !pip install -q six numpy scipy matplotlib scikit-image opencv-python imageio\n# !pip install -q keras imgaug\n# !pip install -q keras",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import keras\nfrom imgaug import augmenters as ia",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "363049cf0704e0aebfa7ea4994c6c291fd66cbac"
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom keras.datasets import cifar10\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.layers import Concatenate\nfrom keras.optimizers import SGD\nfrom keras.callbacks import *\nfrom keras.models import load_model\n\n# from keras.callbacks import LearningRateScheduler,ModelCheckpoint,EarlyStopping,LambdaCallback\nimport os,sys,math",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22f446b53ec82f3244c33dd780ab24b896178cf9"
      },
      "cell_type": "code",
      "source": "import google\ncolab_dir='./'\nfile_name='EIP_CIFAR_10'\nif hasattr(google,'colab'):\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    colab_dir='/content/gdrive/My Drive/Colab Notebooks/'\nmodel_file=colab_dir+file_name+'.h5'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n# backend\nimport tensorflow as tf\nfrom keras import backend as k\n\n# Don't pre-allocate memory; allocate as-needed\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n# Create a session with the above options specified.\nk.tensorflow_backend.set_session(tf.Session(config=config))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa9bb45cb7f0171c15c4b065ade989e82042b15a"
      },
      "cell_type": "code",
      "source": "#Augmentation and resizing\ndef augment(dataset,flip=0.65,blur=2.0,crop=(0,10)):\n    seq = ia.Sequential([ia.Fliplr(flip),ia.GaussianBlur(sigma=(0, blur)),ia.Crop(px=crop)])\n    return seq.augment_images(dataset)\n\ndef augmenter(X,y,start=.25,end=.75):\n    ln=len(X)\n    print('Before augmentation:',X.shape,y.shape)\n    start=int(start*ln)\n    end=int(end*ln)\n    new_X=augment(X)[start:end]\n    new_y=y[start:end]\n    X=np.concatenate((X,new_X))\n    y=np.concatenate((y,new_y))\n    print('After augmentation:',X.shape,y.shape)\n    return (X,y)\n\ndef resize_imgs(imgs,shape=(26,26)):\n    seq = ia.Sequential([ia.Scale({\"height\": shape[0], \"width\": shape[1]})])\n    return seq.augment_images(imgs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b55206328faf39cfbc450beddc20dc5a90aa5218"
      },
      "cell_type": "code",
      "source": "# Dense Block\ndef add_denseblock(input, num_filter = 12, dropout_rate = 0.2,num_layers=12):\n    global compression\n    temp = input\n    for _ in range(num_layers):\n        BatchNorm = BatchNormalization()(temp)\n        relu = Activation('relu')(BatchNorm)\n        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n        if dropout_rate>0:\n            Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])        \n        temp = concat        \n    return temp\n\ndef add_transition(input, num_filter = 12, dropout_rate = 0.2):\n    global compression\n    BatchNorm = BatchNormalization()(input)\n    relu = Activation('relu')(BatchNorm)\n    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n    if dropout_rate>0:\n        Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)    \n    return avg\n\ndef output_layer(input):\n    global compression\n    BatchNorm = BatchNormalization()(input)\n    relu = Activation('relu')(BatchNorm)\n    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n    flat = Flatten()(AvgPooling)\n    output = Dense(num_classes, activation='softmax')(flat)    \n    return output\n  \ndef dense_unit(input_layer,num_filter, dropout_rate,num_layers):\n    dense_block=add_denseblock(input_layer, num_filter, dropout_rate,num_layers)\n    transition_block=add_transition(dense_block, num_filter, dropout_rate)\n    return transition_block\n  \ndef dense_units_chain(n,input_layer,num_filter, dropout_rate,num_layers):\n    dense_unit_=input_layer\n    for i in range(n):\n        dense_unit_=dense_unit(dense_unit_,num_filter, dropout_rate,num_layers)\n    return dense_unit_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d104b63cd5b877c9d8acd0a7ac991f17befa41ea"
      },
      "cell_type": "code",
      "source": "#training time ops\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.lr = []\n    def on_epoch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n        self.lr.append(step_decay(len(self.losses)))\n\ndef step_decay(epoch):\n    initial_lrate = 0.01\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n    return lrate\n\nclass CyclicLR(Callback):\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,new_step_size=None):\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1       \n        K.set_value(self.model.optimizer.lr, self.clr())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d5f083dfa8904a9bd7433e385ad2a0b8a236b22"
      },
      "cell_type": "code",
      "source": "# Load CIFAR10 Data\ndef load_data(resize=False,shape=(26,26)):\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    \n    y_train = keras.utils.to_categorical(y_train, num_classes)\n    y_test = keras.utils.to_categorical(y_test, num_classes)\n    \n    if resize:\n        x_train=resize_imgs(x_train,shape)\n        x_test=resize_imgs(x_test,shape)\n    \n    (x_train, y_train) = augmenter(x_train, y_train,end=1)\n    (x_test, y_test) = augmenter(x_test, y_test,end=1)\n    return (x_train, y_train,x_test, y_test)\n\ndef create_model(input_shape):\n    print('Creating model with input shape',input_shape)\n    input = Input(input_shape)\n    First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n    hidden_dense_blocks = dense_units_chain(n_dense_blocks,First_Conv2D,num_filter,dropout_rate,num_layers)\n    Last_Block = add_denseblock(hidden_dense_blocks, num_filter, dropout_rate)\n    output = output_layer(Last_Block)\n    model = Model(inputs=[input], outputs=[output])\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "991f8e371577a1f2e8197827dab5aac2406ef1b1"
      },
      "cell_type": "code",
      "source": "print('====================HYPER PARAMETERS====================')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02c643e295c0bc677da79c821a40e5b15cc3fdaf"
      },
      "cell_type": "code",
      "source": "# Hyperparameters\nbatch_size = 100\nnum_classes = 10\nmax_epochs = 250\nmodel_small_epochs=1\nmodel_large_epochs=1\n\n#num_layers = 40\nnum_layers = 24\nnum_filter = 12\ncompression = 0.5\ndropout_rate = 0.2\nn_dense_blocks = 3\nsmaller_input=(26,26,3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "275d54ceb5bde42b199b74a7b7fa0b0677874feb"
      },
      "cell_type": "code",
      "source": "print('====================BEGIN OF SMALLER MODEL====================')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4fec52f1aaca7cc57b2152e1b128ed082a482e1b"
      },
      "cell_type": "code",
      "source": "x_train, y_train, x_test, y_test = load_data(resize=True,shape=smaller_input[:-1])\nmodel_small = create_model(x_train.shape[1:])\n\n#callbacks\nmodel_checkpointer=ModelCheckpoint('small_weights.{epoch:02d}-{val_acc:.2f}.h5', monitor='val_acc',\n                verbose=1, save_best_only=True, save_weights_only=False, mode='max', period=5)\n# lrate = LearningRateScheduler(step_decay)\n# early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\nclr = CyclicLR(base_lr=0.1, max_lr=0.2,step_size=8*(len(y_train)/batch_size))\ncallbacks = [clr, model_checkpointer]\n\n\nmodel_small.compile(loss='categorical_crossentropy',metrics=['accuracy'],\n              optimizer=SGD(lr=0.1, decay=1e-6, momentum=0.8, nesterov=True))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c75215750977cd21a9d401a858bb7e892369b7f2",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "model_small.summary()\ntry:\n    model_small.load(model_file)\n    print('Loaded model from file',model_file)\nexcept Exception as e:\n    print('Could not load any model from file')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4aa8818828844c3c353d304c2dd84b2b3e68024"
      },
      "cell_type": "code",
      "source": "model_small.fit(x_train, y_train, batch_size=batch_size,\n                    epochs=model_small_epochs,verbose=1, callbacks=callbacks,\n                    validation_data=(x_test, y_test))\nmodel_small.save(model_file)\nprint('Saved model_small to disk')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86607351bf8f2b7358482e908fe190924650691a"
      },
      "cell_type": "code",
      "source": "print('====================END OF SMALLER MODEL====================')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80cc5c901ceb489ffb260bff35d50ee9af6b1826"
      },
      "cell_type": "code",
      "source": "print('====================BEGIN OF LARGER MODEL====================')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c17f4244c917e97e7cc1557ae60bce8dba7e47e"
      },
      "cell_type": "code",
      "source": "# TRAIN LARGER MODEL\nx_train, y_train, x_test, y_test = load_data(resize=False)\n\n#callbacks\nmodel_checkpointer=ModelCheckpoint('large_weights.{epoch:02d}-{val_acc:.2f}.h5', monitor='val_acc',\n                verbose=1, save_best_only=True, save_weights_only=False, mode='max', period=5)\n# lrate = LearningRateScheduler(step_decay)\n# early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\nclr = CyclicLR(base_lr=0.1, max_lr=0.2,step_size=8*(len(y_train)/batch_size))\ncallbacks = [clr, model_checkpointer]\nmodel_large = create_model(x_train.shape[1:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f3adc0c3037f0d323cda32709d8abd57602b3c3"
      },
      "cell_type": "code",
      "source": "print(os.popen('ls -lia '+colab_dir).read())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84becdcbf2720722640202d3cadb0e690e7708e1"
      },
      "cell_type": "code",
      "source": "try:\n    model_prev = load_model(model_file)\n    print('Loaded model from file',model_file)\nexcept Exception as e:\n    print(e, 'at line ', sys.exc_info()[2].tb_lineno)\n    print('Attempting to load in memory, small model')\n    model_prev=model_small",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ae678b95e0a1b4dfe956fe717ae056a961994932"
      },
      "cell_type": "code",
      "source": "model_prev.input_shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "524ea47999687644666ccdb01fe47f793f447866"
      },
      "cell_type": "code",
      "source": "if model_prev.input_shape[1:]==smaller_input:\n    print('Popping input layer')\n    model_prev.layers.pop(0)\nmodel_large.set_weights(model_prev.get_weights())\nmodel_large.compile(loss='categorical_crossentropy',metrics=['accuracy'],\n                    optimizer=SGD(lr=0.1, decay=1e-6, momentum=0.8, nesterov=True))\nmodel_large.fit(x_train, y_train, batch_size=batch_size,\n                epochs=model_lar_epochs,verbose=1, callbacks=callbacks,\n                validation_data=(x_test, y_test))\nmodel_large.save(model_file)\nprint(\"Saved model to disk\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7be8a1352f65787da27dc9eeb8b9073890561062"
      },
      "cell_type": "code",
      "source": "# Test the model\nscore = model_large.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6151d97fb488aed1553bb9ae29fcb9e3e0bf1f08"
      },
      "cell_type": "code",
      "source": "print('====================END OF LARGER MODEL====================')",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}