{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f67729355a01c19c62bfa2145d9594b1da821fc6"
   },
   "outputs": [],
   "source": [
    "!pip install -q six numpy scipy matplotlib scikit-image opencv-python imageio\n",
    "!pip install -q keras imgaug\n",
    "!pip install -q keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from imgaug import augmenters as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "363049cf0704e0aebfa7ea4994c6c291fd66cbac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import *\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# from keras.callbacks import LearningRateScheduler,ModelCheckpoint,EarlyStopping,LambdaCallback\n",
    "import os,sys,math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22f446b53ec82f3244c33dd780ab24b896178cf9"
   },
   "outputs": [],
   "source": [
    "import google\n",
    "colab_dir='./'\n",
    "file_name='EIP_CIFAR_10'\n",
    "if hasattr(google,'colab'):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    colab_dir='/content/gdrive/My Drive/Colab Notebooks/'\n",
    "model_file=colab_dir+file_name+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa9bb45cb7f0171c15c4b065ade989e82042b15a"
   },
   "outputs": [],
   "source": [
    "#Augmentation and resizing\n",
    "#augment and then concat samples with original\n",
    "def augment(dataset,flip=0.5,blur=1.0,crop=(0,10)):\n",
    "    seq = ia.Sequential([ia.Fliplr(flip),ia.GaussianBlur(sigma=(0, blur)),ia.Crop(px=crop)])\n",
    "    return seq.augment_images(dataset)\n",
    "\n",
    "def augmenter(X,y,start=.25,end=.75):\n",
    "    ln=len(X)\n",
    "    print('Before augmentation:',X.shape,y.shape)\n",
    "    start=int(start*ln)\n",
    "    end=int(end*ln)\n",
    "    new_X=augment(X)[start:end]\n",
    "    new_y=y[start:end]\n",
    "    X=np.concatenate((X,new_X))\n",
    "    y=np.concatenate((y,new_y))\n",
    "    print('After augmentation:',X.shape,y.shape)\n",
    "    return (X,y)\n",
    "\n",
    "#26x26 is almost half of 32x32. 22x22 maybe too small even though its exact half.\n",
    "def resize_imgs(imgs,shape=(26,26)):\n",
    "    seq = ia.Sequential([ia.Scale({\"height\": shape[0], \"width\": shape[1]})])\n",
    "    return seq.augment_images(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b55206328faf39cfbc450beddc20dc5a90aa5218"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2,num_layers=12):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(num_layers):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3),kernel_initializer='he_normal', use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])        \n",
    "        temp = concat        \n",
    "    return temp\n",
    "\n",
    "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1),kernel_initializer='he_normal', use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "        Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)    \n",
    "    return avg\n",
    "\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)    \n",
    "    return output\n",
    "  \n",
    "def dense_unit(input_layer,num_filter, dropout_rate,num_layers):\n",
    "    dense_block=add_denseblock(input_layer, num_filter, dropout_rate,num_layers)\n",
    "    transition_block=add_transition(dense_block, num_filter, dropout_rate)\n",
    "    return transition_block\n",
    "  \n",
    "def dense_units_chain(n,input_layer,num_filter, dropout_rate,num_layers):\n",
    "    dense_unit_=input_layer\n",
    "    for i in range(n):\n",
    "        dense_unit_=dense_unit(dense_unit_,num_filter, dropout_rate,num_layers)\n",
    "    return dense_unit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d104b63cd5b877c9d8acd0a7ac991f17befa41ea"
   },
   "outputs": [],
   "source": [
    "#callback classes\n",
    "#try building model with clr, snapshot ensemble\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"source: https://github.com/bckenstler/CLR/blob/master/clr_callback.py\"\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SnapshotModelCheckpoint(Callback):\n",
    "    \"\"\"source : https://github.com/titu1994/Snapshot-Ensembles/blob/master/snapshot.py\"\"\"\n",
    "    def __init__(self, nb_epochs, nb_snapshots, fn_prefix='Model'):\n",
    "        super(SnapshotModelCheckpoint, self).__init__()\n",
    "\n",
    "        self.check = nb_epochs // nb_snapshots\n",
    "        self.fn_prefix = fn_prefix\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch != 0 and (epoch + 1) % self.check == 0:\n",
    "            filepath = self.fn_prefix + \"-%d.h5\" % ((epoch + 1) // self.check)\n",
    "            self.model.save_weights(filepath, overwrite=True)\n",
    "            #print(\"Saved snapshot at weights/%s_%d.h5\" % (self.fn_prefix, epoch))\n",
    "\n",
    "\n",
    "class SnapshotCallbackBuilder:\n",
    "    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n",
    "        self.T = nb_epochs\n",
    "        self.M = nb_snapshots\n",
    "        self.alpha_zero = init_lr\n",
    "\n",
    "    def get_callbacks(self, model_prefix='Model'):\n",
    "        \"\"\"\n",
    "        Creates a list of callbacks that can be used during training to create a\n",
    "        snapshot ensemble of the model.\n",
    "        Args:\n",
    "            model_prefix: prefix for the filename of the weights.\n",
    "        Returns: list of 3 callbacks [ModelCheckpoint, LearningRateScheduler,\n",
    "                 SnapshotModelCheckpoint] which can be provided to the 'fit' function\n",
    "        \"\"\"\n",
    "        if not os.path.exists('weights/'):\n",
    "            os.makedirs('weights/')\n",
    "\n",
    "        callback_list = [callbacks.ModelCheckpoint(\"weights/%s-Best.h5\" % model_prefix, monitor=\"val_acc\",\n",
    "                                                    save_best_only=True, save_weights_only=True),\n",
    "                         callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule),\n",
    "                         SnapshotModelCheckpoint(self.T, self.M, fn_prefix='weights/%s' % model_prefix)]\n",
    "\n",
    "        return callback_list\n",
    "\n",
    "    def _cosine_anneal_schedule(self, t):\n",
    "        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n",
    "        cos_inner /= self.T // self.M\n",
    "        cos_out = np.cos(cos_inner) + 1\n",
    "        return float(self.alpha_zero / 2 * cos_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85afcd30b9f0ca412bcf7b04938730e52e04bb47"
   },
   "outputs": [],
   "source": [
    "def load_prev_model(model_small=None):\n",
    "#     print(os.popen('du -sh '+colab_dir+'*').read())\n",
    "    last_best=os.popen('du -sh '+colab_dir+r'*|sed -r \"s/^[0-9\\.]+[MK]?\\s(.*)$/\\1/g;\"|tail -1').read().strip()\n",
    "    model_prev=None\n",
    "    try:\n",
    "        print('Attempting to load last best model from file - ',end='')\n",
    "        last_best_fn=last_best.split('/')[-1]\n",
    "        last_best_epoch=last_best_fn.split('.')[1].split('-')[0]\n",
    "        model_prev=load_model(last_best)\n",
    "        print('Sucess\\nLoaded '+last_best_fn, 'epoch :', last_best_epoch)\n",
    "    except Exception as e:\n",
    "        print('Failed!\\n',e)\n",
    "        try:\n",
    "            print('Attempting to load last saved model from file - ',end='')\n",
    "            model_prev = load_model(model_file)\n",
    "            print('Sucess\\nLoaded model from file',model_file)\n",
    "        except Exception as e:\n",
    "    #         print(str(e), 'at line ', sys.exc_info()[2].tb_lineno)\n",
    "            print('Failed!\\n',e)\n",
    "            try:\n",
    "                print('Attempting to load in memory, small model - ',end='')\n",
    "                if len(model_small.layers)>1:\n",
    "                    model_prev=model_small\n",
    "                print('Sucess')\n",
    "            except Exception as e:\n",
    "                print('Failed!\\n',e)\n",
    "    return model_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d5f083dfa8904a9bd7433e385ad2a0b8a236b22"
   },
   "outputs": [],
   "source": [
    "def try_and_copy_weights(model_to,model_from):\n",
    "    #model_to.set_weights(model_from.get_weights())\n",
    "    s,err=0,0\n",
    "    print('Trying to copy weights')\n",
    "    try:\n",
    "        if len(model_to.layers) >1 and len(model_from.layers) >1:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print('Inavlid models',model_to,model_from)\n",
    "        return\n",
    "    for new_layer, layer in zip(model_to.layers[1:], model_from.layers[1:]):\n",
    "        s+=1\n",
    "        try:\n",
    "            new_layer.set_weights(layer.get_weights())\n",
    "            print('.',end='')\n",
    "        except Exception as e:\n",
    "            err+=1\n",
    "    print('Success:',s-err,'Layers , error:',err,'Layers')\n",
    "    \n",
    "#new model with larger i/p layer\n",
    "def larger_model(src_model,shape=(32,32,3)):\n",
    "    new_input=Input(shape)\n",
    "    src_model.layers.pop(0)\n",
    "    new_output=src_model(new_input)\n",
    "    new_model=Model(new_input,new_output)\n",
    "    return new_model\n",
    "\n",
    "# Load CIFAR10 Data\n",
    "def load_data(resize=False,shape=(26,26),test_augment=False):\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    if resize:\n",
    "        x_train=resize_imgs(x_train,shape)\n",
    "        x_test=resize_imgs(x_test,shape)\n",
    "    \n",
    "    (x_train, y_train) = augmenter(x_train, y_train,end=1)\n",
    "    if test_augment:\n",
    "        (x_test, y_test) = augmenter(x_test, y_test,end=1)\n",
    "    return (x_train, y_train,x_test, y_test)\n",
    "#create a dnn model\n",
    "def create_model(input_shape,num_layers,input=None):\n",
    "    print('Creating model with input shape',input_shape)\n",
    "    if input is None:\n",
    "        input = Input(input_shape)\n",
    "    First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "    hidden_dense_blocks = dense_units_chain(n_dense_blocks,First_Conv2D,num_filter,dropout_rate,num_layers)\n",
    "    Last_Block = add_denseblock(hidden_dense_blocks, num_filter, dropout_rate)\n",
    "    output = output_layer(Last_Block)\n",
    "    model = Model(inputs=[input], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "991f8e371577a1f2e8197827dab5aac2406ef1b1"
   },
   "outputs": [],
   "source": [
    "print('====================HYPER PARAMETERS====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02c643e295c0bc677da79c821a40e5b15cc3fdaf"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "max_epochs = 250\n",
    "model_small_epochs=1\n",
    "model_large_epochs1=160\n",
    "# model_large_epochs2=60\n",
    "\n",
    "num_layers = 40\n",
    "# layers_small=12\n",
    "# layers_large=num_layers-layers_small\n",
    "layers_large=layers_small=num_layers\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2\n",
    "n_dense_blocks = 3\n",
    "smaller_input=(26,26,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "275d54ceb5bde42b199b74a7b7fa0b0677874feb"
   },
   "outputs": [],
   "source": [
    "print('====================BEGIN OF SMALLER MODEL====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4fec52f1aaca7cc57b2152e1b128ed082a482e1b"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "x_train, y_train, x_test, y_test = load_data(resize=True,shape=smaller_input[:-1])\n",
    "\n",
    "#callbacks\n",
    "model_checkpointer=ModelCheckpoint('small_weights.{epoch:02d}-{val_acc:.2f}.h5', monitor='val_acc',\n",
    "                verbose=1, save_best_only=True, save_weights_only=False, mode='max', period=2)\n",
    "# early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "clr = CyclicLR(base_lr=0.1, max_lr=0.2,step_size=8*(len(y_train)/batch_size))\n",
    "\n",
    "M = 6\n",
    "nb_epoch = T = 200\n",
    "alpha_zero = 0.1\n",
    "model_prefix = 'ensemble_'\n",
    "snapshot = SnapshotCallbackBuilder(T, M, alpha_zero) \n",
    "\n",
    "\n",
    "# callbacks = snapshot.get_callbacks(model_prefix=model_prefix)\n",
    "callbacks_lst = [clr, model_checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1c8f577fd7d55a94eea1620db6e4a4d0356ec81"
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "model_small = create_model((x_train.shape[1:]),num_layers=layers_small)\n",
    "model_small.compile(loss='categorical_crossentropy',metrics=['accuracy'],\n",
    "              optimizer=SGD(lr=0.1, decay=1e-4, momentum=0.9, nesterov=True))\n",
    "model_small.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4aa8818828844c3c353d304c2dd84b2b3e68024",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train model\n",
    "hs=model_small.fit(x_train, y_train, batch_size=batch_size,verbose=1,\n",
    "                    epochs=model_small_epochs, callbacks=callbacks,\n",
    "                    validation_data=(x_test, y_test))\n",
    "model_small.save(model_file)\n",
    "print('Saved model_small to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86607351bf8f2b7358482e908fe190924650691a"
   },
   "outputs": [],
   "source": [
    "print('====================END OF SMALLER MODEL====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80cc5c901ceb489ffb260bff35d50ee9af6b1826"
   },
   "outputs": [],
   "source": [
    "print('====================BEGIN OF LARGER MODEL====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c17f4244c917e97e7cc1557ae60bce8dba7e47e"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "x_train, y_train, x_test, y_test = load_data(resize=False)\n",
    "\n",
    "# callbacks\n",
    "# model_checkpointer=ModelCheckpoint('large_weights.{epoch:02d}-{val_acc:.2f}.h5', monitor='val_acc',\n",
    "#                 verbose=1, save_best_only=True, save_weights_only=False, mode='max', period=2)\n",
    "# early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "clr = CyclicLR(base_lr=0.1, max_lr=4,step_size=8*(len(y_train)/batch_size),mode='triangular2',gamma=0.99994)\n",
    "\n",
    "M = 6\n",
    "nb_epoch = T = 200\n",
    "alpha_zero = 0.1\n",
    "model_prefix = 'ensemble_'\n",
    "snapshot = SnapshotCallbackBuilder(T, M, alpha_zero) \n",
    "\n",
    "\n",
    "# callbacks = snapshot.get_callbacks(model_prefix=model_prefix)\n",
    "callbacks_lst = [clr, model_checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f3adc0c3037f0d323cda32709d8abd57602b3c3"
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "# model_prev=load_prev_model(model_small)\n",
    "\n",
    "model_large = create_model(x_train.shape[1:],num_layers=layers_large)\n",
    "# model_large = larger_model(model_prev)\n",
    "\n",
    "model_large.compile(loss='categorical_crossentropy',metrics=['accuracy'],\n",
    "                    optimizer=SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True))\n",
    "model_large.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53b7ed0b0e8acefefda51802f0bd74f0cace4c72"
   },
   "outputs": [],
   "source": [
    "#train model\n",
    "hl=model_large.fit(x_train, y_train, batch_size=batch_size, verbose=1,\n",
    "                epochs=model_large_epochs1, callbacks=callbacks,\n",
    "                validation_data=(x_test, y_test))\n",
    "model_large.save(model_file)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7be8a1352f65787da27dc9eeb8b9073890561062"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "score = model_large.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6151d97fb488aed1553bb9ae29fcb9e3e0bf1f08"
   },
   "outputs": [],
   "source": [
    "print('====================END OF LARGER MODEL====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_large.evaluate(x_test, y_test, verbose=1)\n",
    "s=open('res.txt','w')\n",
    "s.write(score[0],score[1])\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=open('res.txt','a')\n",
    "s.write(str(hl))\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
